{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YoloV2 - detection de chiffres avec le MNIST\n",
    "\n",
    "Notebook de correction du TP.\n",
    "\n",
    "## Génération des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_DATASETS = \"datasets/\"\n",
    "PATH_MNIST = os.path.join(PATH_DATASETS, \"source\")\n",
    "PATH_TRAIN_SET = os.path.join(PATH_DATASETS, \"train_set\")\n",
    "PATH_VAL_SET = os.path.join(PATH_DATASETS, \"validation_set\")\n",
    "\n",
    "if not os.path.exists(PATH_DATASETS):\n",
    "    os.mkdir(PATH_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_dataset import generate_dataset\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "print(\"Generating train set...\")\n",
    "generate_dataset(\n",
    "    source_dataset=MNIST(PATH_MNIST, train=True, download=True),\n",
    "    path_dataset=PATH_TRAIN_SET,\n",
    "    sample_count=5000,\n",
    "    image_width=512,\n",
    "    image_height=512,\n",
    "    min_objects_per_image=0,\n",
    "    max_objects_per_image=20,\n",
    "    noise_strength=0\n",
    ")\n",
    "\n",
    "print(\"Generating test set...\")\n",
    "generate_dataset(\n",
    "    source_dataset=MNIST(PATH_MNIST, train=False, download=True),\n",
    "    path_dataset=PATH_VAL_SET,\n",
    "    sample_count=500,\n",
    "    image_width=512,\n",
    "    image_height=512,\n",
    "    min_objects_per_image=0,\n",
    "    max_objects_per_image=20,\n",
    "    noise_strength=0\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.display_boxes import display_random_dataset_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "images = display_random_dataset_samples(\n",
    "    path_dataset=PATH_TRAIN_SET,\n",
    "    sample_count=1\n",
    ")\n",
    "\n",
    "for image in images:\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.mobilenetv2 import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor, nn\n",
    "import numpy as np\n",
    "\n",
    "from yolo.dataset import YoloDataset, collate_fn\n",
    "from yolo.metrics import CocoEvaluator\n",
    "from yolo.post_processor import non_maximum_suppression, filter_by_score\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction d'activation\n",
    "\n",
    "Le format du tenseur `encoded_boxes` avant l'activation:\n",
    "- `x[..., 0] = x` : position x du centre de la boîte par rapport a sa cellule.\n",
    "- `x[..., 1] = y` : position y du centre de la boîte par rapport a sa cellule.\n",
    "- `x[..., 2] = w` : largeur de la boîte par rapport a son prior.\n",
    "- `x[..., 3] = h` : hauteur de la boîte par rapport a son prior.\n",
    "- `x[..., 4] = L` : score d'objectivité (probabilité que la boîte contienne un objet, logit pré-activation).\n",
    "- `x[..., 5:] = Lc` : probabilités conditionelle des classes (logit pré-activation).\n",
    "\n",
    "Le format du tenseur de sortie:\n",
    "- `y[..., 0] = x` : position x du centre de la boîte par rapport a la feature map.\n",
    "- `y[..., 1] = y` : position y du centre de la boîte par rapport a la feature map.\n",
    "- `y[..., 2] = w` : largeur de la boîte par rapport a la feature map.\n",
    "- `y[..., 3] = h` : hauteur de la boîte par rapport a la feature map.\n",
    "- `y[..., 4] = P` : score d'objectivité.\n",
    "- `y[..., 5:] = C` : probabilités conditionelle des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_boxes(\n",
    "    encoded_boxes: Tensor,\n",
    "    priors: Tensor,\n",
    "    input_size: tuple[int, int]\n",
    ") -> Tensor:\n",
    "    \"\"\"Decode the output bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        encoded_boxes (Tensor): Encoded bounding boxes.\n",
    "        priors (Tensor): Priors of shape (P x 2).\n",
    "        input_size (tuple[int, int]): Shape of the input image.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Decoded bounding boxes.\n",
    "    \"\"\"\n",
    "    grid_h, grid_w = encoded_boxes.shape[1:3]\n",
    "    scale_h, scale_w = input_size[0] / grid_h, input_size[1] / grid_w\n",
    "\n",
    "    # The predicted coordinates are local to the cell of the prediction, meaning it\n",
    "    # is a value between 0 and 1, positioning the center of the box within that\n",
    "    # cell. The goal of the activation function is to get the coordinates of the box\n",
    "    # in relative to the grid (i.e. x in [0 WG[ and y in [0 HG[). To do this, first\n",
    "    # we create a tensor in which each cell contains its own coordinates within the\n",
    "    # grid (shape: (1 x HG x WG x 1 x 2)).\n",
    "    cell_w = torch.arange(0, grid_w, dtype=torch.float32).to(encoded_boxes.device)\n",
    "    cell_h = torch.arange(0, grid_h, dtype=torch.float32).to(encoded_boxes.device)\n",
    "    cell_grid = torch.stack(torch.meshgrid(cell_w, cell_h, indexing=\"ij\"))\n",
    "    cell_grid = torch.swapaxes(cell_grid, 0, -1)\n",
    "    cell_grid = torch.reshape(cell_grid, shape=(1, grid_h, grid_w, 1, 2))\n",
    "\n",
    "    # Then, we add the coordinates of the cell to the coordinates of the box after\n",
    "    # applying the sigmoid function (shape: (BS x HG x WG x P x 2)).\n",
    "    predicted_xy = encoded_boxes[..., :2]\n",
    "    predicted_xy = ...  # TODO\n",
    "\n",
    "    # The predicted dimension of the box is relative to the associated prior. The\n",
    "    # activation function used here is the exponential function, meaning that a\n",
    "    # predicted size of 0 pre-activation will give the size of the prior (shape:\n",
    "    # (BS x HG x WG x P x 2)).\n",
    "    predicted_wh = encoded_boxes[..., 2:4]\n",
    "    predicted_wh = ...  # TODO\n",
    "\n",
    "    # Since the priors are relative to the input image size, we have to rescale the\n",
    "    # boxes to the grid.\n",
    "    ...  # TODO\n",
    "\n",
    "    # The predicted objectness is merely obtained by applying the sigmoid function\n",
    "    # to the logit. This probability indicates whether the box actually contains an\n",
    "    # object. The loss function used by YOLOv2 will make this value quantify the\n",
    "    # quality of the box (shape: (BS x HG x WG x P x 1)).\n",
    "    predicted_objectness = encoded_boxes[..., 4]\n",
    "    predicted_objectness = ...  # TODO\n",
    "\n",
    "    # Finally, the conditional probability vector is obtained by applying the\n",
    "    # softmax function (shape: (BS x HG x WG x P x C])).\n",
    "    predicted_probabilities = encoded_boxes[..., 5:]\n",
    "    predicted_probabilities = ...  # TODO\n",
    "\n",
    "    # The output tensor is then assembled by concatenating the previously computed\n",
    "    # tensors (shape: (BS x HG x WG x P x (5+C))).\n",
    "    decoded_boxes = torch.cat([...], axis=-1)  # TODO\n",
    "\n",
    "    return decoded_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition de la tête de detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDetectionHead(nn.Module):\n",
    "    \"\"\"Yolo detection head.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        priors: Tensor | np.ndarray,\n",
    "        num_classes: int\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the detection head.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Depth of the input feature map.\n",
    "            priors (Tensor | np.ndarray): Pre-computed priors.\n",
    "            num_classes (int): Number of output class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.prior_count = priors.shape[0]\n",
    "\n",
    "        if isinstance(priors, np.ndarray):\n",
    "            priors = torch.from_numpy(priors)\n",
    "\n",
    "        self.register_buffer(\"priors\", priors)\n",
    "\n",
    "        self.convolution_head = ...  # TODO\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_maps: Tensor,\n",
    "        input_size: tuple[int, int]\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Compute bounding boxes based on the given feature map.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (Tensor): Feature extractor output tensor.\n",
    "            input_size (tuple[int, int]): Input sizes of each image of the batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Decoded bounding boxes.\n",
    "        \"\"\"\n",
    "        encoded_boxes = self.convolution_head(feature_maps)\n",
    "\n",
    "        # The decode function expects to receive a BS x HG x WG x P x (5+C) tensor.\n",
    "        # Since torch uses a channels first format, we have a BS x [B*(5+C)] x HG x WG\n",
    "        # tensor at this step, so we have to move the axes around. First, we switch from\n",
    "        # channels first to channels last and then split the channels into priors x\n",
    "        # classes.\n",
    "        encoded_boxes_reshaped = torch.moveaxis(encoded_boxes, 1, -1)\n",
    "        encoded_boxes_reshaped = torch.reshape(encoded_boxes_reshaped, shape=(\n",
    "            *encoded_boxes_reshaped.shape[:-1], self.prior_count, 5 + self.num_classes\n",
    "        ))\n",
    "\n",
    "        decoded_boxes = decode_boxes(encoded_boxes_reshaped, self.priors, input_size)\n",
    "\n",
    "        return decoded_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût\n",
    "\n",
    "Formats des tenseurs de sorties et GT (shape: BS x HG x WG x P x (5+C)):\n",
    "- `y[..., 0] = x` : position x du centre de la boîte par rapport a la feature map.\n",
    "- `y[..., 1] = y` : position y du centre de la boîte par rapport a la feature map.\n",
    "- `y[..., 2] = w` : largeur de la boîte par rapport a la feature map.\n",
    "- `y[..., 3] = h` : hauteur de la boîte par rapport a la feature map.\n",
    "- `y[..., 4] = P` : score d'objectivité.\n",
    "- `y[..., 5:] = C` : probabilités conditionelle des classes.\n",
    "\n",
    "Pour les GT, toutes les valeurs sont à 0 si il n'y pas de boîte dans la cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_yolo(predicted: Tensor, ground_truth: Tensor) -> Tensor:\n",
    "    \"\"\"Compute the Yolo loss.\n",
    "\n",
    "    Args:\n",
    "        predicted (Tensor): The predicted tensor.\n",
    "        ground_truth (Tensor): The ground truth tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The loss tensor.\n",
    "    \"\"\"\n",
    "    # First, we split the predicted tensors to retrieve the coordinates and the\n",
    "    # conditional probabilities.\n",
    "    predicted_xy = predicted[..., :2]  # (BS x HG x WG x P x 2)\n",
    "    predicted_wh = predicted[..., 2:4]  # (BS x HG x WG x P x 2)\n",
    "    predicted_objectness = predicted[..., 4]  # (BS x HG x WG x B)\n",
    "    predicted_probabilities = predicted[..., 5:]  # (BS x HG x WG x P x C)\n",
    "\n",
    "    # Same for the ground truth tensors.\n",
    "    true_xy = ground_truth[..., :2]  # (BS x HG x WG x P x 2)\n",
    "    true_wh = ground_truth[..., 2:4]  # (BS x HG x WG x P x 2)\n",
    "    true_objectness = ground_truth[..., 4]  # (BS x HG x WG x B)\n",
    "    true_probabilities = ground_truth[..., 5:]  # (BS x HG x WG x P x C)\n",
    "\n",
    "    # Position error: a simple square error between the predicted and ground truth\n",
    "    # positions.\n",
    "    diff_xy = ...  # TODO\n",
    "\n",
    "    # Dimension error: a square error between the square roots of the predicted and\n",
    "    # ground truth dimensions.\n",
    "    diff_wh = ...  # TODO\n",
    "\n",
    "    # The following operations consists in determining the IOU between the predicted and\n",
    "    # ground truth boxes. First, we compute the position of top-left and bottom-right\n",
    "    # corners of the predicted boxes.\n",
    "    predicted_x0_y0 = predicted_xy - predicted_wh / 2\n",
    "    predicted_x1_y1 = predicted_xy + predicted_wh / 2\n",
    "\n",
    "    # Same goes for the ground truth boxes.\n",
    "    true_x0_y0 = true_xy - true_wh / 2\n",
    "    true_x1_y1 = true_xy + true_wh / 2\n",
    "\n",
    "    # Then we compute the coordinates of the intersection between the predicted and\n",
    "    # ground truth boxes.\n",
    "    intersection_x0_y0 = torch.maximum(predicted_x0_y0, true_x0_y0)\n",
    "    intersection_x1_y1 = torch.minimum(predicted_x1_y1, true_x1_y1)\n",
    "\n",
    "    # Using the coordinates, we can deduce the dimensions of the intersection.\n",
    "    # If the intersection is empty, at least one of the dimension will be negative. By\n",
    "    # setting it to zero, the intersection area will be zero.\n",
    "    intersection_wh = intersection_x1_y1 - intersection_x0_y0\n",
    "    intersection_wh = torch.clamp(intersection_wh, min=0)\n",
    "\n",
    "    # Then, we compute the intersection area between the predicted and ground truth\n",
    "    # boxes.\n",
    "    intersection_area = ...  # TODO\n",
    "\n",
    "    # To compute the IOU we also need to compute the union area.\n",
    "    predicted_area = ...  # TODO\n",
    "    true_area = ...  # TODO\n",
    "    union_area = ...  # TODO\n",
    "\n",
    "    # Finally, we compute the IOU between the predicted and ground truth boxes.\n",
    "    iou_scores = ...  # TODO\n",
    "\n",
    "    # Objectness error: a square error between the predicted objectness and the IOU\n",
    "    # between the boxes. This means that the objectness will tend to quantify the\n",
    "    # quality of the predicted box.\n",
    "    diff_objectness = ...  # TODO\n",
    "\n",
    "    # No objectness error: if the predicted box does not contain an object, the\n",
    "    # objectness should tend to zero.\n",
    "    diff_no_object = ...  # TODO\n",
    "\n",
    "    # Classification error: a square error between the predicted and ground truth\n",
    "    # conditional probabilities. Note that any kind of classification loss can be used\n",
    "    # such as the binary cross-entropy.\n",
    "    diff_classification = ...  # TODO\n",
    "\n",
    "    # The total loss is the weighted sum of all the previously computed errors.\n",
    "    diff = ...  # TODO\n",
    "\n",
    "    diff = torch.sum(diff, dim=(1, 2, 3))\n",
    "    loss = torch.mean(diff)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo.detection_head import YoloDetectionHead\n",
    "from yolo.ground_truth import YoloAnnotation, generate_ground_truth_tensors\n",
    "from yolo.loss_fn import loss_yolo\n",
    "from yolo.post_processor import DetectionResult, decode_boxes as box_tensor_to_box_list\n",
    "\n",
    "class YoloDetector(nn.Module):\n",
    "    \"\"\"Yolo detector model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        priors: np.ndarray,\n",
    "        num_classes: int,\n",
    "        feature_map_depth: int = 1280\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the model.\n",
    "\n",
    "        Args:\n",
    "            backbone (nn.Module): Feature extractor.\n",
    "            priors (np.ndarray): Model priors.\n",
    "            num_classes (int): Number of output class.\n",
    "            feature_map_depth (int): Depth of the feature extractor output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.priors = priors\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.detection_head = ...  # TODO\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: Tensor,\n",
    "        targets: list[list[YoloAnnotation]] | None = None\n",
    "    ) -> Tensor | list[DetectionResult]:\n",
    "        \"\"\"Run the detector.\n",
    "\n",
    "        This function behavior differ between training and evaluation.\n",
    "        - when training, the targets (ground truth) must be supplied, and the loss is\n",
    "          directly returned.\n",
    "        - when not training, the decoded bounding boxes are returned (the boxes are not\n",
    "          filtered yet, so more post-processing such as NMS may be required).\n",
    "        This is done this way since it is how it is done in the torchvision library.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): Input images (must be batched).\n",
    "            targets (list[list[YoloAnnotation]], optional): Used in training mode, the\n",
    "                ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor | list[DetectionResult]: Either the loss over the batch in training\n",
    "                mode, or the detected boxes.\n",
    "        \"\"\"\n",
    "        input_size = images.shape[2:]\n",
    "        batched_box_tensors = self.detection_head(self.backbone(images), input_size)\n",
    "\n",
    "        if self.training:\n",
    "            assert targets is not None, \"Targets are required when training.\"\n",
    "\n",
    "            ground_truth = generate_ground_truth_tensors(\n",
    "                annotations=targets,\n",
    "                priors=self.priors,\n",
    "                input_size=input_size,\n",
    "                grid_size=batched_box_tensors.shape[1:3],\n",
    "                num_classes=self.detection_head.num_classes\n",
    "            ).to(images.device)\n",
    "\n",
    "            return loss_yolo(batched_box_tensors, ground_truth)\n",
    "\n",
    "        return [\n",
    "            box_tensor_to_box_list(box_tensor, input_size)\n",
    "            for box_tensor in batched_box_tensors\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des datasets et calcul des priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_annotations = os.path.join(PATH_TRAIN_SET, \"annotations.json\")\n",
    "path_train_images = os.path.join(PATH_TRAIN_SET, \"images\")\n",
    "\n",
    "path_val_annotations = os.path.join(PATH_VAL_SET, \"annotations.json\")\n",
    "path_val_images = os.path.join(PATH_VAL_SET, \"images\")\n",
    "\n",
    "train_dataset = YoloDataset(path_train_annotations, path_train_images)\n",
    "val_dataset = YoloDataset(path_val_annotations, path_val_images)\n",
    "\n",
    "priors = train_dataset.get_priors(cluster_count=3)\n",
    "\n",
    "print(\"Priors utilisés:\")\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle\n",
    "\n",
    "On utilise le feature extractor de MobileNetV2, il a l'avantage d'être plutot léger. Sa feature map contient 1280 caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = YoloDetector(\n",
    "    backbone=mobilenet_v2(\n",
    "        weights=MobileNet_V2_Weights.DEFAULT,\n",
    "        progress=True\n",
    "    ).features,\n",
    "    priors=priors,\n",
    "    num_classes=10,\n",
    "    feature_map_depth=1280  # Profondeur de la feature map du MobileNet\n",
    ").to(DEVICE)\n",
    "\n",
    "detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boucle de train et validation\n",
    "\n",
    "Note: un modèle de type `YoloDetector` change de comportement entre le train et le test:\n",
    "- Pendant le train, appeler le modèle en passant les GT retourne directement la loss.\n",
    "- Pendant le test, le modèle retourne les boîtes décodées.\n",
    "\n",
    "Les détections sont représentées sour la forme d'un dictionnaire\n",
    "- `\"boxes\"`: les bounding boxes dans le format _xywh_\n",
    "- `\"scores\"`: un vecteur des scores de chaque boîte\n",
    "- `\"labels\"`: un vecteur des labels de chaque boîte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    detector: YoloDetector,\n",
    "    data_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    image_transforms: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> float:\n",
    "    \"\"\"Run a single train epoch.\n",
    "\n",
    "    Args:\n",
    "        detector (YoloDetector): Yolo model.\n",
    "        data_loader (DataLoader): Torch data loader.\n",
    "        optimizer (Optimizer): Torch optimizer.\n",
    "        image_transforms (nn.Module): Transforms applied to the image.\n",
    "        device (torch.device): Model device.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss over the epoch.\n",
    "    \"\"\"\n",
    "    detector.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        images = image_transforms(images)\n",
    "\n",
    "        loss = detector(images, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def validation_step(\n",
    "    detector: YoloDetector,\n",
    "    data_loader: DataLoader,\n",
    "    image_transforms: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    scheduler: optim.lr_scheduler.LRScheduler | None = None\n",
    ") -> float:\n",
    "    \"\"\"Run a single validation step.\n",
    "\n",
    "    Args:\n",
    "        detector (YoloDetector): Yolo model.\n",
    "        data_loader (DataLoader): Torch data loader.\n",
    "        image_transforms (nn.Module): Transforms applied to the image.\n",
    "        device (torch.device): Model device.\n",
    "        scheduler (LRScheduler, optional): Learning scheduler\n",
    "\n",
    "    Returns:\n",
    "        float: mAP over the validation set.\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "\n",
    "    # L'objet ci-dessous est un wrapper autour de la lib `pycocotools`. pycocotools est\n",
    "    # une library qui n'est pas très pratique à utiliser, mais implemente les fonctions\n",
    "    # pour le calcul de la mAP, ce wrapper permet d'eviter tout le boilerplate\n",
    "    # necessaire.\n",
    "    evaluator = CocoEvaluator()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            images = image_transforms(images)\n",
    "\n",
    "            detections = detector(images)\n",
    "\n",
    "            for detection, target in zip(detections, targets):\n",
    "                evaluator.add_detections(detection, target)\n",
    "\n",
    "    metrics = evaluator.compute_metrics()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(metrics)\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrage de l'entrainement\n",
    "\n",
    "On utilise Adam comme optimizer, avec un scheduler pour réduire le learning rate lorsque la mAP n'augmente plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12  # A choisir en fonction de la GPU\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "optimizer = optim.Adam(detector.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\")\n",
    "\n",
    "# Valeurs de normalisation utilisées par le MobileNet.\n",
    "image_transforms = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "best_checkpoint = None\n",
    "best_map = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    loss = train_step(detector, train_loader, optimizer, image_transforms, DEVICE)\n",
    "    metrics = validation_step(detector, val_loader, image_transforms, DEVICE, scheduler)\n",
    "\n",
    "    print(f\"Train loss over the epoch: {loss:.2f}\")\n",
    "    print(f\"Validation mAP: {metrics:.2f}\")\n",
    "\n",
    "    if metrics > best_map:\n",
    "        best_map = metrics\n",
    "        torch.save(detector.state_dict(), \"checkpoint.pth\")\n",
    "\n",
    "print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.load_state_dict(torch.load(\"checkpoint.pth\"))\n",
    "detector.eval()\n",
    "\n",
    "metrics = validation_step(detector, val_loader, image_transforms, DEVICE, scheduler)\n",
    "print(f\"Validation mAP: {metrics:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.display_boxes import display_image_with_boxes\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "path_image = os.path.join(path_val_images, random.choice(os.listdir(path_val_images)))\n",
    "\n",
    "image = cv2.imread(path_image)\n",
    "image_torch = torch.from_numpy(image).to(torch.float32)\n",
    "image_torch = torch.moveaxis(image_torch, -1, 0) / 255\n",
    "image_torch = image_torch.unsqueeze(0)\n",
    "image_torch = image_transforms(image_torch)\n",
    "image_torch = image_torch.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    detector.eval()\n",
    "    detections = detector(image_torch)[0]\n",
    "\n",
    "detections = filter_by_score(detections, 0.3)\n",
    "detections = non_maximum_suppression(detections)\n",
    "\n",
    "image_with_detections = display_image_with_boxes(\n",
    "    image,\n",
    "    boxes=detections[\"boxes\"],\n",
    "    labels=detections[\"labels\"],\n",
    "    scores=detections[\"scores\"],\n",
    "    mode=\"xywh\",\n",
    "    display_labels=True,\n",
    "    scaling_factor=5\n",
    ")\n",
    "\n",
    "plt.imshow(image_with_detections)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
